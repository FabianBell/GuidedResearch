@article{DBLP:journals/corr/abs-1901-08149,
  author    = {Thomas Wolf and
               Victor Sanh and
               Julien Chaumond and
               Clement Delangue},
  title     = {TransferTransfo: {A} Transfer Learning Approach for Neural Network
               Based Conversational Agents},
  journal   = {CoRR},
  volume    = {abs/1901.08149},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.08149},
  archivePrefix = {arXiv},
  eprint    = {1901.08149},
  timestamp = {Tue, 02 Jun 2020 12:48:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-08149.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{liu2020impress,
    title={You Impress Me: Dialogue Generation via Mutual Persona Perception},
    author={Qian Liu and Yihong Chen and Bei Chen and Jian-Guang Lou and Zixuan Chen and Bin Zhou and Dongmei Zhang},
    year={2020},
    eprint={2004.05388},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{riley2020textsettr,
      title={TextSETTR: Label-Free Text Style Extraction and Tunable Targeted Restyling},
      author={Parker Riley and Noah Constant and Mandy Guo and Girish Kumar and David Uthus and Zarana Parekh},
      year={2020},
      eprint={2010.03802},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{peng2020soloist,
      title={SOLOIST: Few-shot Task-Oriented Dialog with A Single Pre-trained Auto-regressive Model},
      author={Baolin Peng and Chunyuan Li and Jinchao Li and Shahin Shayandeh and Lars Liden and Jianfeng Gao},
      year={2020},
      eprint={2005.05298},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{DBLP:journals/corr/abs-1907-00448,
  author    = {Jiawei Wu and
               Xin Wang and
               William Yang Wang},
  title     = {Self-Supervised Dialogue Learning},
  journal   = {CoRR},
  volume    = {abs/1907.00448},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.00448},
  archivePrefix = {arXiv},
  eprint    = {1907.00448},
  timestamp = {Tue, 26 Nov 2019 08:07:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-00448.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/ml/Williams92,
  author    = {Ronald J. Williams},
  title     = {Simple Statistical Gradient-Following Algorithms for Connectionist
               Reinforcement Learning},
  journal   = {Mach. Learn.},
  volume    = {8},
  pages     = {229--256},
  year      = {1992},
  url       = {https://doi.org/10.1007/BF00992696},
  doi       = {10.1007/BF00992696},
  timestamp = {Mon, 02 Mar 2020 16:28:58 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/Williams92.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@techreport{raffel2019exploring,
title	= {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
author	= {Adam Roberts and Colin Raffel and Katherine Lee and Michael Matena and Noam Shazeer and Peter J. Liu and Sharan Narang and Wei Li and Yanqi Zhou},
year	= {2019},
institution	= {Google}
}
@article{wolf2019huggingface,
  title={HuggingFace's Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}
@misc{budzianowski2020multiwoz,
      title={MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling},
      author={Paweł Budzianowski and Tsung-Hsien Wen and Bo-Hsiang Tseng and Iñigo Casanueva and Stefan Ultes and Osman Ramadan and Milica Gašić},
      year={2020},
      eprint={1810.00278},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{byrne2019taskmaster,
  title={Taskmaster-1: Toward a realistic and diverse dialog dataset},
  author={Byrne, Bill and Krishnamoorthi, Karthik and Sankar, Chinnadhurai and Neelakantan, Arvind and Duckworth, Daniel and Yavuz, Semih and Goodrich, Ben and Dubey, Amit and Cedilnik, Andy and Kim, Kyu-Young},
  journal={arXiv preprint arXiv:1909.05358},
  year={2019}
}
@article{li2020dgst,
  title={DGST: a dual-generator network for text style transfer},
  author={Li, Xiao and Chen, Guanyi and Lin, Chenghua and Li, Ruizhe},
  journal={arXiv preprint arXiv:2010.14557},
  year={2020}
}
@article{li2018delete,
  title={Delete, retrieve, generate: A simple approach to sentiment and style transfer},
  author={Li, Juncen and Jia, Robin and He, He and Liang, Percy},
  journal={arXiv preprint arXiv:1804.06437},
  year={2018}
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}
@inproceedings{mcauley2013amateurs,
  title={From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews},
  author={McAuley, Julian John and Leskovec, Jure},
  booktitle={Proceedings of the 22nd international conference on World Wide Web},
  pages={897--908},
  year={2013}
}
@inproceedings{ni2019justifying,
  title={Justifying recommendations using distantly-labeled reviews and fine-grained aspects},
  author={Ni, Jianmo and Li, Jiacheng and McAuley, Julian},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={188--197},
  year={2019}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
