#!/usr/bin/env python
import os
import logging
import argparse
import time

import pandas as pd
from bellparallel import parallel
import re

pattern = re.compile(r'[^{]*{\s\s?(?P<data>[^}]*)\s}(\squery\s{\s(?P<query>.*)\s})?\s<EOB>\s*')

@parallel(tag='Evaluate')
def evaluate(entry):
    segments = entry.segments
    if entry.task == 'belief_state':
        if len(segments) == 0:
            return
        prediction = entry.prediction
        match = pattern.fullmatch(prediction)
        if match is None:
            return 0
        bs = {k: v for k, v in [pair for pair in [elem.split(' = ') for elem in match.group('data').split('; ') if elem != ''] if len(pair) == 2]}
        score = 0
        for key in segments:
            if key in bs:
                if bs[key] == segments[key]:
                    score += 1
        score = score / len(segments)
        return score
    elif entry.task == 'response':
        if len(segments) == 0:
            return
        score = 0
        for key in segments:
            if segments[key] in entry.prediction:
                score += 1
        score = score / len(segments)
        return score
    else:
        raise ValueError(f'Invalid task type: {entry.task}')

def main(args):
    data = pd.read_csv(args.data)
    dataset = pd.read_json(args.dataset)

    dataset = dataset[dataset.columns[2:]]
    data = pd.concat([data, dataset], axis=1)

    out = evaluate(data.iloc, length=len(data))
    out = [(*args, score) for args, score in zip(data.values, out)]
    data = pd.DataFrame(out, columns=[*data.columns, 'inform_success'])
    data.to_json('result.json')
    
    print(f'BLEU: {data.score.sum() / len(data)}')
    
    resp = data[(data.task == 'response') & data.inform_success.notnull()]
    print(f'Success: {resp.inform_success.sum() / len(resp)}')
    bel = data[(data.task == 'belief_state') & data.inform_success.notnull()]
    print(f'Inform: {bel.inform_success.sum() / len(bel)}')

def dir_path(path):
    """
    Type check for argparse
    """
    if os.path.isdir(path):
        return path
    raise NotADirectoryError(path)

def file_path(path):
    """
    Type check of argparse
    """
    if os.path.isfile(path):
        return path
    raise FileNotFoundError(path)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Evaluate model results')
    
    parser.add_argument('--verbose', '-v', dest='verbose', action='store_true')
    parser.add_argument('data', type=file_path, help='Data path (.json)')
    parser.add_argument('dataset', type=file_path, help='Dataset path the model was evaluated on. The same order is required.')

    args = parser.parse_args()
    if args.verbose is True:
        logging.basicConfig(level=logging.INFO)
    start = time.time()
    logging.info('Start.')
    out = main(args)
    end = time.time()
    logging.info('Finished. Time: {}'.format(end-start))
