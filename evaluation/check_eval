#!/usr/bin/env python
import os
import logging
import argparse
import time

import pandas as pd
from tqdm import tqdm
import re

pattern = re.compile(r'[^{]*{\s\s?(?P<data>[^}]*)\s}(\squery\s{\s(?P<query>.*)\s})?\s<EOB>\s*')

def evaluate(data):
    prev_responses = None
    resp_inp = None
    for entry in tqdm(data.iloc, total=len(data), desc='Evaluate'):
        segments = entry.segments
        if entry.task == 'belief_state':
            if len(segments) == 0:
                yield
                continue
            prediction = entry.prediction
            if not isinstance(prediction, str):
                yield 0
                continue
            match = pattern.fullmatch(prediction)
            if match is None:
                yield 0
                continue
            bs = {k: v for k, v in [pair for pair in [elem.split(' = ') for elem in match.group('data').split('; ') if elem != ''] if len(pair) == 2]}
            score = 0
            for key in segments:
                if key in bs:
                    if bs[key] == segments[key]:
                        score += 1
            score = score / len(segments)
            yield score
        elif entry.task == 'response':
            sub_seq = entry.input.split(' =>')[0]
            assert (resp_inp is None) == (prev_responses is None)
            if resp_inp is None or sub_seq not in resp_inp:
               prev_responses = entry.prediction
            else:
                prev_responses += entry.prediction
            resp_inp = sub_seq
            if len(segments) == 0:
                yield
                continue
            score = 0
            for key in segments:
                if segments[key] in prev_responses:
                    score += 1
            score = score / len(segments)
            yield score
        else:
            raise ValueError(f'Invalid task type: {entry.task}')

def main(args):
    data = pd.read_csv(args.data)
    dataset = pd.read_json(args.dataset)

    dataset = dataset[dataset.columns[2:]]
    data = pd.concat([data, dataset], axis=1)
    out = evaluate(data)
    out = [(*args, score) for args, score in zip(data.values, out)]
    data = pd.DataFrame(out, columns=[*data.columns, 'inform_success'])
    data.to_json('result.json')
    
    print(f'BLEU: {data.score.sum() / len(data)}')
    
    resp = data[(data.task == 'response') & data.inform_success.notnull()]
    print(f'Success: {resp.inform_success.sum() / len(resp)}')
    bel = data[(data.task == 'belief_state') & data.inform_success.notnull()]
    print(f'Inform: {bel.inform_success.sum() / len(bel)}')

def dir_path(path):
    """
    Type check for argparse
    """
    if os.path.isdir(path):
        return path
    raise NotADirectoryError(path)

def file_path(path):
    """
    Type check of argparse
    """
    if os.path.isfile(path):
        return path
    raise FileNotFoundError(path)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Evaluate model results')
    
    parser.add_argument('--verbose', '-v', dest='verbose', action='store_true')
    parser.add_argument('data', type=file_path, help='Data path (.json)')
    parser.add_argument('dataset', type=file_path, help='Dataset path the model was evaluated on. The same order is required.')

    args = parser.parse_args()
    if args.verbose is True:
        logging.basicConfig(level=logging.INFO)
    start = time.time()
    logging.info('Start.')
    out = main(args)
    end = time.time()
    logging.info('Finished. Time: {}'.format(end-start))
